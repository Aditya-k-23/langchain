{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialization and Deserialization of Memory Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain now supports the serialization of memory types that store memory using ChatMessageHistory. This enables the conversion of the memory type into a JSON file that can be stored locally and can be loaded back using the deserialization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from tests.unit_tests.llms.fake_llm import FakeLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization using `to_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized Memory: \n",
      "{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'memory', 'buffer', 'ConversationBufferMemory'], 'kwargs': {}, 'obj': {'ai_prefix': 'AI', 'chat_memory': {'id': ['langchain', 'memory', 'chat_message_histories', 'in_memory', 'ChatMessageHistory'], 'kwargs': {}, 'lc': 1, 'obj': {'messages': [{'data': {'additional_kwargs': {}, 'content': 'hi', 'example': False, 'type': 'human'}, 'type': 'human'}, {'data': {'additional_kwargs': {}, 'content': \"what's up\", 'example': False, 'type': 'ai'}, 'type': 'ai'}, {'data': {'additional_kwargs': {}, 'content': 'not much you', 'example': False, 'type': 'human'}, 'type': 'human'}, {'data': {'additional_kwargs': {}, 'content': 'not much', 'example': False, 'type': 'ai'}, 'type': 'ai'}]}, 'type': 'constructor'}, 'human_prefix': 'Human', 'input_key': None, 'memory_key': 'history', 'output_key': None, 'return_messages': False}}\n"
     ]
    }
   ],
   "source": [
    "# This example uses Conversation Buffer Memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"what's up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "print(\"Serialized Memory: \")\n",
    "serialized = memory.to_json()\n",
    "print(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deserialization using `from_json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# This example takes the JSON created above and deserializes it into a Conversation Buffer Memory object\n",
    "deserialized = ConversationBufferMemory.from_json(json.dumps(serialized))\n",
    "\n",
    "print(deserialized == memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `ConversationSummaryMemory`, `ConversationSummaryBufferMemory`, and `ConversationEntityMemory` `from_json` requires an additional argument of an `llm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lc': 1, 'type': 'constructor', 'id': ['langchain', 'memory', 'summary', 'ConversationSummaryMemory'], 'kwargs': {'llm': 'FakeLLM'}, 'obj': {'ai_prefix': 'AI', 'buffer': 'foo', 'chat_memory': {'id': ['langchain', 'memory', 'chat_message_histories', 'in_memory', 'ChatMessageHistory'], 'kwargs': {}, 'lc': 1, 'obj': {'messages': [{'data': {'additional_kwargs': {}, 'content': 'hi', 'example': False, 'type': 'human'}, 'type': 'human'}, {'data': {'additional_kwargs': {}, 'content': 'what is up', 'example': False, 'type': 'ai'}, 'type': 'ai'}]}, 'type': 'constructor'}, 'human_prefix': 'Human', 'input_key': None, 'llm': {'id': ['tests', 'unit_tests', 'llms', 'fake_llm', 'FakeLLM'], 'lc': 1, 'repr': 'FakeLLM()', 'type': 'not_implemented'}, 'memory_key': 'history', 'output_key': None, 'prompt': {'id': ['langchain_core', 'prompts', 'prompt', 'PromptTemplate'], 'kwargs': {'input_variables': ['new_lines', 'summary'], 'template': 'Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:', 'template_format': 'f-string'}, 'lc': 1, 'type': 'constructor'}, 'return_messages': False, 'summary_message_cls': 'SystemMessage'}}\n"
     ]
    }
   ],
   "source": [
    "# This example uses FakeLLM, it can be replaced with any other LLM\n",
    "summary_memory = ConversationSummaryMemory(llm=FakeLLM())\n",
    "\n",
    "summary_memory.save_context({\"input\": \"hi\"}, {\"output\": \"what is up\"})\n",
    "summary_memory.load_memory_variables({})\n",
    "\n",
    "serialized_summary_memory = summary_memory.to_json()\n",
    "print(serialized_summary_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "llm = FakeLLM()\n",
    "json_str = json.dumps(serialized_summary_memory)\n",
    "revived_obj = ConversationSummaryMemory.from_json(json_str, llm=llm)\n",
    "print(revived_obj == summary_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the memory type in JSON uses an `llm` that does not match with the `llm` passed into the `from_json` function in the form of the JSON string, the `from_json` function will provide a warning and continue the deserialization with the newly passed `llm`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
